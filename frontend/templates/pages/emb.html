{% extends 'layouts/services.html' %}
{% block title %}Home{% endblock %}
{% block content %}

<div class="page-header">
  <h1>Text Representations</h1>
  <p> <u>Description Of the service</u></p>
<p>In Text Representation module we find word / sentence embeddings where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network. The main benefit of the dense representations is generalization power allowing words or sentences with similar meaning to have a similar representation.
Google’s BERT and recent transformer-based methods hav outperformed the state-of-the-art on several tasks. Several models have been presented on Bert ,to  either improve its prediction metrics or computational speed, but not both. It is difficult to use such large models underlow latency constraints. Here we are providing pre-trained models of BERT and two variants of BERT - DistilBert and AlBert</u></p>
</div>


<form action="{{ url_for('services.emb_call') }}" method="post">

<label for="text">Input text here:</label>
<textarea id="txtarea" class="form-control" rows="4" name="txt_ip">{{ input_text}}
</textarea>

<br>
<label for="text">Select Model type :</label>
<select id="id" name="id">
<option value ="100" name ="pruned">Bert Embeddings</option>
<option value ="101" name ="pruned">Distil Bert Embeddings</option>
<option value ="102" name ="pruned">AlBert Embeddings</option>
</select>
<br>
<input style="margin-top:0.5cm" class="btn" id="generate"type="submit" value="Generate Embeddings"/>


</form>

<br>
<form id="output">
	<label for="text">Sentiment</label>
  <textarea class="form-control" id="output_text" rows="3"> {{ output }}</textarea>
</form>


<hr>

<div class="page-header">
  <p><u>API Details</u></p>
	<p>We have included three popular models for Text Representation here:</p>

	<p>BERT 		- Is a bi-directional transformer for pre-training over a lot of unlabeled textual data to learn a language representation that can be used to fine-tune for 	                   specific machine learning tasks.
	</p>
	<p>
	DistilBert 	- Is a variant of Bert that improves on the inference speed. It has minor degradation on the performance but greatly reduces the training time.
	</p>
	<p>
	AlBert 		- Is BERT’s successor, a much smaller/smarter Lite Bert called ALBERT. It's strength lies in the dramatic reduction in model/parameter size, thereby making them 					  light-weight, responsive and energy-efficient models.  
	</p>
	<p>
    The API accepts the 'Text' for which representation is desired and the 'Model' name as inputs. It combines them into a dictionary and converts it into a .json file.
	The .json file is received by the server , Text and Model details are extracted and the corresponding model is called for generating Embeddings. The Embeddings are again written to a .json file.</p>
</div>


{% endblock %}
