{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import nltk, re, pickle\n",
    "from sklearn.externals import joblib\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import pickle as pkl\n",
    "import os, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11536</th>\n",
       "      <td>@someUSER im going to bali tomorrow for the we...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11537</th>\n",
       "      <td>@someUSER i guess we'll see how far we've come...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>bola hai admin  ,  aankhe kholo duites  ,  kno...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>get your sweat suits !  it's november and its ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>maa sab samajti hein c&gt; mom always know what y...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text     label\n",
       "11536  @someUSER im going to bali tomorrow for the we...   Neutral\n",
       "11537  @someUSER i guess we'll see how far we've come...  Positive\n",
       "11538  bola hai admin  ,  aankhe kholo duites  ,  kno...   Neutral\n",
       "11539  get your sweat suits !  it's november and its ...  Positive\n",
       "11540  maa sab samajti hein c> mom always know what y...   Neutral"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readData(dataFilePath):\n",
    "    textData = open(dataFilePath, \"r\")\n",
    "    textDataset = []    \n",
    "    for line in textData.readlines():\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        textDataset.append(line)\n",
    "    return textDataset\n",
    "        \n",
    "dataFilePath = \"Data/newData.txt\"\n",
    "textDataSet = readData(dataFilePath)\n",
    "textDataFrame = pd.DataFrame(textDataSet, columns=['text', 'label'])\n",
    "textDataFrame.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11541 11541\n"
     ]
    }
   ],
   "source": [
    "xData = list(textDataFrame.text)\n",
    "xLabel = list(textDataFrame.label)\n",
    "print(len(xData), len(xLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contractionMapping.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractionMapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                       \"here's\": \"here is\",\n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "acronyms = {}\n",
    "extraAcronyms = [\"'t\", 'not', 'nt', 'not', 'sec', 'seconds', \"'d\", 'had', \"'ve\", 'have', \"'ll\", \"will\", \"'s\", 'is', 'af', 'as fuck', 'fb', 'facebook', 'C', 'see','u', 'you', 'gd', 'good', 'n8', 'night', 'LOL','Laughing Out Loud','BRB','Be Right Back','BBL','Be Back Later','ASAP','As Soon As Possible','B4','Before','B4N','Bye For Now','BTW','By The Way','B/C', 'Because','C U L8R','See You Later','Def','Definitely','tmrw','Tomorrow','ETA','Estimated Time of Arrival','F2F','Face To Face', 'GF', 'Girlfriend', 'BF', 'Boyfriend','BFF','Best Friend Forever','AML','All My Love','GTG', 'Got To Go', 'G2G','Got To Go', 'GR8', 'Great', 'IDK', 'I Don’t Know', 'IDC', 'I Don’t Care', 'ILY', 'I Love You', 'ILU','I Love You','JK','Just Kidding','K','OK','LMK','Let Me Know','THX','Thanks','PLS','Please','TTYL','Talk To You Later','TMI','Too Much Information','CYE','Check Your Email','FYI','For Your Information','IMHO','In My Honest Opinion','ROFL','Rolling On the Floor Laughing','BION','Believe It Or Not','BRT','Be Right There','Y','Why?','PCM','Please Call Me','IYKWIM','If You Know What I Mean','WYSIWYG','What You See Is What You Get','XOXO','Hugs and Kisses', 'ur', 'your', 'tmrw', 'tomorrow', 'omg', 'oh my god', 'ab', 'about', 'abt', 'about', 'adventuritter', 'adventurous', 'b', 'be', 'beetweet', 'hot tweet', 'BFN', 'bye for now', 'bgd', 'background', 'BR', 'best regards', 'BTW', 'by the way', 'chk', 'check', 'cld', 'could', 'clk', 'click', 'cre8', 'create', 'da', 'the', 'deets', 'details',  'dm', 'direct message', 'egotwistical', 'egotistical and Twitter', 'EM', 'email', 'eml', 'e-mail', 'EMA', 'e-mail address', 'F2F', 'face to face', 'fab', 'fabulous', 'FAV', 'favorite', 'FOMO', 'fear of missing out', 'FTL', 'for the loss', 'FTW', 'for the win', 'IC', 'I see', 'ICYMI', 'in case you missed it', 'MRT', 'modified retweet', 'MTF', 'more to follow', 'NTS', 'note to self', 'OH', 'overheard', 'SMH', 'shaking my head', 'SP', 'Sponsored', 'TBH', 'to be honest', 'TBT', 'Throw Back Thursday', 'TFTF', 'thanks for the follow', 'TMB', 'Tweet me back', 'woz',  'was', 'wtv',  'whatever', 'ykyat', 'you know you’re addicted to', 'YOLO', 'you only live once', 'YOYO', 'you’re on your own']\n",
    "c=0\n",
    "for i in range(0, len(extraAcronyms)-1, 2):\n",
    "    key = extraAcronyms[c].lower()\n",
    "    value = extraAcronyms[c+1].lower()\n",
    "    acronyms[key] = value\n",
    "    c+=2\n",
    "acronyms.update(contractionMapping)\n",
    "listAcronyms = list(acronyms)\n",
    "joblib.dump(acronyms, 'Pickles/contractionMapping.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11541,) (11541,)\n"
     ]
    }
   ],
   "source": [
    "def findAcronyms(acronym):\n",
    "    if(acronym in listAcronyms):\n",
    "        return acronyms[acronym]\n",
    "    else:\n",
    "        return acronym\n",
    "\n",
    "def dataPreprocess(data):\n",
    "    preprocessedSentence = []\n",
    "    for sentence in data:\n",
    "        newTokens = []\n",
    "        removetable = str.maketrans('', '', string.punctuation)\n",
    "        removetable1 = str.maketrans('', '', string.digits)\n",
    "        wordsOfSentence = sentence.split()\n",
    "        for word in wordsOfSentence:\n",
    "            word = word.lower()\n",
    "            word = findAcronyms(word)\n",
    "            word = word.translate(removetable)\n",
    "            word = word.translate(removetable1)\n",
    "            if(word.startswith('@')!=True and word.startswith('<')!=True and word.startswith('www')!=True and word.startswith('http')!=True and word.endswith('com')!=True):\n",
    "                if(len(word)>1):\n",
    "                    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "                    newTokens.append(word)\n",
    "        if(len(newTokens)==0):\n",
    "            wordsOfSentence = [x.lower() for x in wordsOfSentence]\n",
    "            preprocessedSentence.append(wordsOfSentence)\n",
    "            #print(wordsOfSentence)\n",
    "        else:\n",
    "            preprocessedSentence.append(newTokens)\n",
    "    return preprocessedSentence\n",
    "\n",
    "textFeaturesOld = dataPreprocess(xData)\n",
    "textFeatures_ = [' '.join(i) for i in textFeaturesOld]\n",
    "textFeatures = np.asarray(textFeatures_)\n",
    "labels = np.asarray(xLabel)\n",
    "print(textFeatures.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Neutral': 4529, 'Positive': 4239, 'Negative': 2773})\n"
     ]
    }
   ],
   "source": [
    "#No of samples in each class\n",
    "\n",
    "counter = Counter(labels)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10963, 5000) (578, 5000)\n"
     ]
    }
   ],
   "source": [
    "#Data split into train and test sets\n",
    "\n",
    "xTrain, xValid, xTrainLabels, xValidLabels = model_selection.train_test_split(textFeatures, labels, test_size=0.05, random_state=11)\n",
    "tfidfVectorObject = TfidfVectorizer(analyzer='word', max_features=5000)\n",
    "tfidfVectorObject.fit(textFeatures)\n",
    "joblib.dump(tfidfVectorObject, 'Pickles/tfidfFeatureVector.pkl')\n",
    "xTrainTfidfVectors =  tfidfVectorObject.transform(xTrain).toarray()\n",
    "xValidTfidfVectors =  tfidfVectorObject.transform(xValid).toarray()\n",
    "print(xTrainTfidfVectors.shape, xValidTfidfVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results\n",
    "\n",
    "def trainModel(classifier, trainFetureVector, trainLabel, validFeatureVector):\n",
    "    classifier.fit(trainFetureVector, trainLabel)\n",
    "    joblib.dump(classifier, 'Pickles/mlpCodeMixedModel.pkl')\n",
    "    predictedLabels = classifier.predict(validFeatureVector)\n",
    "    f1Score = f1_score(xValidLabels, predictedLabels, average='macro')\n",
    "    print(\"f1-score macro: \",f1Score)\n",
    "    print(classification_report(xValidLabels, predictedLabels))\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.03836361\n",
      "Iteration 2, loss = 0.81351422\n",
      "Iteration 3, loss = 0.62702347\n",
      "f1-score macro:  0.6206214233358214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.60      0.51      0.55       134\n",
      "     Neutral       0.67      0.69      0.68       241\n",
      "    Positive       0.62      0.65      0.63       203\n",
      "\n",
      "    accuracy                           0.63       578\n",
      "   macro avg       0.63      0.62      0.62       578\n",
      "weighted avg       0.63      0.63      0.63       578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Multi Layer Perceptron Before Data Balancing\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(300,), max_iter=3, solver='adam', learning_rate_init=0.001, verbose=10, random_state=1)\n",
    "classifier = trainModel(mlp, xTrainTfidfVectors, xTrainLabels, xValidTfidfVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12864, 5000) (12864,)\n",
      "Counter({'Positive': 4288, 'Neutral': 4288, 'Negative': 4288})\n"
     ]
    }
   ],
   "source": [
    "#Data Classes Balancing\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ROS = RandomOverSampler(random_state=11)\n",
    "overSampledTfidfText, overSampledTfidfLabels = ROS.fit_sample(xTrainTfidfVectors, xTrainLabels)\n",
    "print(overSampledTfidfText.shape, overSampledTfidfLabels.shape)\n",
    "print(Counter(overSampledTfidfLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.04788366\n",
      "Iteration 2, loss = 0.84274815\n",
      "Iteration 3, loss = 0.66609193\n",
      "f1-score macro:  0.6471867119832249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.59      0.63      0.61       134\n",
      "     Neutral       0.68      0.69      0.68       241\n",
      "    Positive       0.68      0.62      0.65       203\n",
      "\n",
      "    accuracy                           0.65       578\n",
      "   macro avg       0.65      0.65      0.65       578\n",
      "weighted avg       0.66      0.65      0.65       578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Multi Layer Perceptron After Data Balancing\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=3, solver='adam', learning_rate_init=0.001, verbose=10, random_state=1)\n",
    "classifier = trainModel(mlp, overSampledTfidfText, overSampledTfidfLabels, xValidTfidfVectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
